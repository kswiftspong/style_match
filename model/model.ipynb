{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K\n",
    "from keras import applications\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16 - Frozen, 2 dense layers (4096), classification layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 936 images belonging to 4 classes.\n",
      "Found 312 images belonging to 4 classes.\n",
      "Epoch 1/40\n",
      "59/58 [==============================] - 142s 2s/step - loss: 20.9458 - categorical_accuracy: 0.2831 - val_loss: 2.2803 - val_categorical_accuracy: 0.4295\n",
      "Epoch 2/40\n",
      "59/58 [==============================] - 141s 2s/step - loss: 1.5821 - categorical_accuracy: 0.3643 - val_loss: 1.7500 - val_categorical_accuracy: 0.5096\n",
      "Epoch 3/40\n",
      "59/58 [==============================] - 141s 2s/step - loss: 1.4235 - categorical_accuracy: 0.4156 - val_loss: 1.2820 - val_categorical_accuracy: 0.4615\n",
      "Epoch 4/40\n",
      "59/58 [==============================] - 140s 2s/step - loss: 1.3305 - categorical_accuracy: 0.4466 - val_loss: 1.5764 - val_categorical_accuracy: 0.5897\n",
      "Epoch 5/40\n",
      "59/58 [==============================] - 141s 2s/step - loss: 1.2403 - categorical_accuracy: 0.4829 - val_loss: 1.1880 - val_categorical_accuracy: 0.5705\n",
      "Epoch 6/40\n",
      "59/58 [==============================] - 141s 2s/step - loss: 1.1953 - categorical_accuracy: 0.5128 - val_loss: 1.0588 - val_categorical_accuracy: 0.5865\n",
      "Epoch 7/40\n",
      "59/58 [==============================] - 141s 2s/step - loss: 1.1488 - categorical_accuracy: 0.5438 - val_loss: 0.6880 - val_categorical_accuracy: 0.5737\n",
      "Epoch 8/40\n",
      "59/58 [==============================] - 141s 2s/step - loss: 1.1476 - categorical_accuracy: 0.5406 - val_loss: 0.9540 - val_categorical_accuracy: 0.4679\n",
      "Epoch 9/40\n",
      "59/58 [==============================] - 140s 2s/step - loss: 1.0905 - categorical_accuracy: 0.5759 - val_loss: 1.0857 - val_categorical_accuracy: 0.6346\n",
      "Epoch 10/40\n",
      "59/58 [==============================] - 141s 2s/step - loss: 1.1279 - categorical_accuracy: 0.5759 - val_loss: 0.9885 - val_categorical_accuracy: 0.5801\n",
      "Epoch 11/40\n",
      "59/58 [==============================] - 141s 2s/step - loss: 1.1134 - categorical_accuracy: 0.5737 - val_loss: 1.2505 - val_categorical_accuracy: 0.6506\n",
      "Epoch 12/40\n",
      "59/58 [==============================] - 141s 2s/step - loss: 1.0641 - categorical_accuracy: 0.5641 - val_loss: 1.3074 - val_categorical_accuracy: 0.6891\n",
      "Epoch 13/40\n",
      "59/58 [==============================] - 140s 2s/step - loss: 1.0597 - categorical_accuracy: 0.5641 - val_loss: 1.4644 - val_categorical_accuracy: 0.5385\n",
      "Epoch 14/40\n",
      "59/58 [==============================] - 140s 2s/step - loss: 1.0423 - categorical_accuracy: 0.5962 - val_loss: 1.1053 - val_categorical_accuracy: 0.6795\n",
      "Epoch 15/40\n",
      "59/58 [==============================] - 140s 2s/step - loss: 1.0443 - categorical_accuracy: 0.6026 - val_loss: 1.5683 - val_categorical_accuracy: 0.5833\n",
      "Epoch 16/40\n",
      "59/58 [==============================] - 140s 2s/step - loss: 1.0792 - categorical_accuracy: 0.6068 - val_loss: 1.0367 - val_categorical_accuracy: 0.6026\n",
      "Epoch 17/40\n",
      "59/58 [==============================] - 139s 2s/step - loss: 1.0217 - categorical_accuracy: 0.5972 - val_loss: 1.0373 - val_categorical_accuracy: 0.7147\n",
      "Epoch 18/40\n",
      "59/58 [==============================] - 140s 2s/step - loss: 1.0779 - categorical_accuracy: 0.6122 - val_loss: 1.5062 - val_categorical_accuracy: 0.6699\n",
      "Epoch 19/40\n",
      "59/58 [==============================] - 139s 2s/step - loss: 1.0292 - categorical_accuracy: 0.6154 - val_loss: 0.7904 - val_categorical_accuracy: 0.5833\n",
      "Epoch 20/40\n",
      "59/58 [==============================] - 140s 2s/step - loss: 0.9468 - categorical_accuracy: 0.6357 - val_loss: 0.9668 - val_categorical_accuracy: 0.7051\n",
      "Epoch 21/40\n",
      "59/58 [==============================] - 140s 2s/step - loss: 0.9589 - categorical_accuracy: 0.6335 - val_loss: 1.0281 - val_categorical_accuracy: 0.7340\n",
      "Epoch 22/40\n",
      "59/58 [==============================] - 140s 2s/step - loss: 1.0292 - categorical_accuracy: 0.6346 - val_loss: 1.1460 - val_categorical_accuracy: 0.6923\n",
      "Epoch 23/40\n",
      "59/58 [==============================] - 140s 2s/step - loss: 0.9587 - categorical_accuracy: 0.6432 - val_loss: 0.9624 - val_categorical_accuracy: 0.6955\n",
      "Epoch 24/40\n",
      "59/58 [==============================] - 140s 2s/step - loss: 1.0301 - categorical_accuracy: 0.6261 - val_loss: 0.8206 - val_categorical_accuracy: 0.6378\n",
      "Epoch 25/40\n",
      "59/58 [==============================] - 141s 2s/step - loss: 0.9598 - categorical_accuracy: 0.6571 - val_loss: 1.2546 - val_categorical_accuracy: 0.5962\n",
      "Epoch 26/40\n",
      "59/58 [==============================] - 140s 2s/step - loss: 0.9304 - categorical_accuracy: 0.6325 - val_loss: 0.9561 - val_categorical_accuracy: 0.7212\n",
      "Epoch 27/40\n",
      "59/58 [==============================] - 140s 2s/step - loss: 0.9978 - categorical_accuracy: 0.6410 - val_loss: 1.0640 - val_categorical_accuracy: 0.6410\n",
      "Epoch 28/40\n",
      "59/58 [==============================] - 140s 2s/step - loss: 0.9494 - categorical_accuracy: 0.6549 - val_loss: 0.9627 - val_categorical_accuracy: 0.6827\n",
      "Epoch 29/40\n",
      "59/58 [==============================] - 140s 2s/step - loss: 0.9839 - categorical_accuracy: 0.6335 - val_loss: 1.0456 - val_categorical_accuracy: 0.7147\n",
      "Epoch 30/40\n",
      "59/58 [==============================] - 140s 2s/step - loss: 1.0239 - categorical_accuracy: 0.6442 - val_loss: 1.0278 - val_categorical_accuracy: 0.6955\n",
      "Epoch 31/40\n",
      "59/58 [==============================] - 140s 2s/step - loss: 0.9736 - categorical_accuracy: 0.6346 - val_loss: 1.0402 - val_categorical_accuracy: 0.6667\n",
      "Epoch 32/40\n",
      "59/58 [==============================] - 140s 2s/step - loss: 0.9428 - categorical_accuracy: 0.6346 - val_loss: 0.7310 - val_categorical_accuracy: 0.7083\n",
      "Epoch 33/40\n",
      "59/58 [==============================] - 140s 2s/step - loss: 0.9099 - categorical_accuracy: 0.6656 - val_loss: 0.8780 - val_categorical_accuracy: 0.6667\n",
      "Epoch 34/40\n",
      "59/58 [==============================] - 140s 2s/step - loss: 0.9610 - categorical_accuracy: 0.6400 - val_loss: 0.8878 - val_categorical_accuracy: 0.6987\n",
      "Epoch 35/40\n",
      "59/58 [==============================] - 139s 2s/step - loss: 0.9542 - categorical_accuracy: 0.6560 - val_loss: 1.0529 - val_categorical_accuracy: 0.7436\n",
      "Epoch 36/40\n",
      "59/58 [==============================] - 139s 2s/step - loss: 0.9928 - categorical_accuracy: 0.6389 - val_loss: 0.9386 - val_categorical_accuracy: 0.6859\n",
      "Epoch 37/40\n",
      "59/58 [==============================] - 140s 2s/step - loss: 0.9070 - categorical_accuracy: 0.6464 - val_loss: 0.8560 - val_categorical_accuracy: 0.6699\n",
      "Epoch 38/40\n",
      "59/58 [==============================] - 139s 2s/step - loss: 0.9559 - categorical_accuracy: 0.6720 - val_loss: 0.7270 - val_categorical_accuracy: 0.6731\n",
      "Epoch 39/40\n",
      "59/58 [==============================] - 139s 2s/step - loss: 0.9639 - categorical_accuracy: 0.6560 - val_loss: 0.9862 - val_categorical_accuracy: 0.6154\n",
      "Epoch 40/40\n",
      "59/58 [==============================] - 139s 2s/step - loss: 0.9800 - categorical_accuracy: 0.6656 - val_loss: 0.8007 - val_categorical_accuracy: 0.6667\n"
     ]
    }
   ],
   "source": [
    "def vgg_tl():\n",
    "    img_width, img_height = 224, 224\n",
    "\n",
    "    train_data_dir = '../data/split_images/train'\n",
    "    validation_data_dir = '../data/split_images/validation'\n",
    "    epochs = 40\n",
    "    batch_size = 16\n",
    "\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        input_shape = (3, img_width, img_height)\n",
    "    else:\n",
    "        input_shape = (img_width, img_height, 3)\n",
    "\n",
    "    # Load VGG16 without the top layers\n",
    "    vgg_conv_base = applications.VGG16(include_top = False, weights = 'imagenet', input_shape = input_shape)\n",
    "\n",
    "    # Freeze the VGG16 base layers (make them non-trainable)\n",
    "    for layer in vgg_conv_base.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Create new model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add the VGG16 base to the new model\n",
    "    model.add(vgg_conv_base)\n",
    "\n",
    "    # Add the top layers (Flatten, Dense 64, Dense 4) with a dropout between the dense layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(4096))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(4))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['categorical_accuracy'])\n",
    "\n",
    "    train_datagen = ImageDataGenerator(\n",
    "          rescale=1./255,\n",
    "          rotation_range=20,\n",
    "          width_shift_range=0.2,\n",
    "          height_shift_range=0.2,\n",
    "          horizontal_flip=True,\n",
    "          fill_mode='nearest')\n",
    "\n",
    "    validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "            train_data_dir,\n",
    "            target_size=(img_width, img_height),\n",
    "            batch_size=batch_size,\n",
    "            class_mode='categorical')\n",
    "\n",
    "    validation_generator = validation_datagen.flow_from_directory(\n",
    "            validation_data_dir,\n",
    "            target_size=(img_width, img_height),\n",
    "            batch_size=batch_size,\n",
    "            class_mode='categorical',\n",
    "            shuffle=False)\n",
    "\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit_generator(\n",
    "          train_generator,\n",
    "          steps_per_epoch=train_generator.samples/train_generator.batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=validation_generator,\n",
    "          validation_steps=validation_generator.samples/validation_generator.batch_size,\n",
    "          verbose=1)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = vgg_tl()\n",
    "model.save('model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
